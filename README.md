AI Image Generation using Diffusion Models

This project showcases how to generate stunning AI-generated images from text prompts using Stable Diffusion v1.5 and Hugging Face Diffusers. Diffusion models begin with random noise and gradually refine it into a clear image, guided by your text input.

They are widely used for AI art, realistic image synthesis, creative design, and video generation.

ğŸš€ Features

Text-to-image generation with Stable Diffusion v1.5

Works on GPU (CUDA), Apple Silicon (MPS), and CPU

High-quality image sampling using Euler Ancestral Scheduler

Supports negative prompts to filter unwanted artifacts

Reproducible results with manual seeding

ğŸ› ï¸ Installation

Install the required dependencies:

diffusers

transformers

accelerate

torch & torchvision

safetensors

âš¡ Workflow

Set up the pipeline â†’ Load the Stable Diffusion model and detect your hardware (GPU/CPU/MPS).

Define your prompts â†’ Describe what you want and specify what you donâ€™t want (negative prompts).

Generate the image â†’ Fine-tune quality with parameters like inference steps, guidance scale, and random seeds.

Save the result â†’ Export as .png or any format of your choice.

âš™ï¸ Key Parameters

num_inference_steps â†’ Controls detail and refinement (higher = more detail, slower).

guidance_scale â†’ Adjusts how strictly the model follows your text prompt.

seed â†’ Ensures reproducibility; change it for new variations.
